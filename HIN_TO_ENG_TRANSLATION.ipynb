{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63305bc1",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a80f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from unicodedata import normalize\n",
    "import sys\n",
    "import indicnlp\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33e1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open('ENG_HIN_SMALL_DATASET.txt' , mode ='rt' , encoding = 'utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a92a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(text):\n",
    "    lines = text.strip().split('\\n') #1d list of strings\n",
    "    eng_hin_pairs = [line.split('\\t') for line in lines] #list\n",
    "    eng_hin_pairs = np.delete(eng_hin_pairs , -1 , axis = 1)\n",
    "    eng_hin_pairs = eng_hin_pairs.tolist()\n",
    "    return eng_hin_pairs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7cf6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('ENG_HIN_SMALL_DATASET.txt')\n",
    "hin_eng_pairs = to_pairs(doc)\n",
    "english_sentences = []\n",
    "hindi_sentences = []\n",
    "for i in range(len(hin_eng_pairs)):\n",
    "    english_sentences.append(hin_eng_pairs[i][0])\n",
    "    hindi_sentences.append(hin_eng_pairs[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598fb62",
   "metadata": {},
   "source": [
    "### Cleaning english data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english_data(lines):\n",
    "    re_print = re.compile('[^%s]' %re.escape(string.printable))\n",
    "    table = str.maketrans('' , '' , string.punctuation)\n",
    "    clean_eng_lines = []\n",
    "    for line in lines:\n",
    "        line = normalize('NFD' , line).encode('ascii' , 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        line = line.split()\n",
    "        line = [word.lower() for word in line]\n",
    "        line = [word.translate(table) for word in line]\n",
    "        line = [re_print.sub('' , w) for w in line]\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        line = ' '.join(line)\n",
    "        clean_eng_lines.append(line)\n",
    "    return clean_eng_lines   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80f3ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Wow!', 'Help!', 'Jump.', 'Jump.', 'Jump.', 'Hello!', 'Hello!', 'Cheers!', 'Cheers!', 'Got it?']\n",
      "['wow', 'help', 'jump', 'jump', 'jump', 'hello', 'hello', 'cheers', 'cheers', 'got it']\n"
     ]
    }
   ],
   "source": [
    "clean_eng_lines = clean_english_data(english_sentences)\n",
    "print(type(clean_eng_lines))\n",
    "print(english_sentences[0:10])\n",
    "print(clean_eng_lines[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c25c9",
   "metadata": {},
   "source": [
    "### Clean hindi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62fcdf",
   "metadata": {},
   "source": [
    "###### SETTING UP PATHS FOR INDIC NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIC_NLP_LIB_HOME=r\"C:\\Users\\sudha\\Desktop\\NMT_PROJECTS\\Language_Translation_Chat_Bot\\anoopkunchukuttan-indic_nlp_library-eccde81\"\n",
    "INDIC_NLP_RESOURCES=r\"C:\\Users\\sudha\\Desktop\\NMT_PROJECTS\\Language_Translation_Chat_Bot\\indic_nlp_resources-master\"\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa7ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(line):\n",
    "    text = line\n",
    "    text = text.replace(u',' ,'')\n",
    "    text = text.replace(u'\"' ,'')\n",
    "    text = text.replace(u'(' ,'')\n",
    "    text = text.replace(u')' ,'')\n",
    "    text = text.replace(u'\"' ,'')\n",
    "    text = text.replace(u':' ,'')\n",
    "    text = text.replace(u\"'\" ,'')\n",
    "    text = text.replace(u\"'\" ,'')\n",
    "    text=text.replace(u\"‘‘\",'')\n",
    "    text=text.replace(u\"’’\",'')\n",
    "    text=text.replace(u\"''\",'')\n",
    "    text=text.replace(u\".\",'')\n",
    "    text=text.replace(u\"-\",'')\n",
    "    text=text.replace(u\"।\",'')\n",
    "    text=text.replace(u\"?\",'')\n",
    "    text=text.replace(u\"\\\\\",'')\n",
    "    text=text.replace(u\"_\",'')\n",
    "    text = re.sub('[a-zA-Z]' , '' , text)\n",
    "    text = re.sub('[0-9+\\-*/.%&!]' , '' , text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4128f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hindi_data(lines):\n",
    "    clean_hin_lines = []\n",
    "    for line in lines:\n",
    "        remove_nuktas = False\n",
    "        factory = IndicNormalizerFactory()\n",
    "        normalizer = factory.get_normalizer(\"hi\" , remove_nuktas = False )\n",
    "        line = clean_text(line)\n",
    "        tokens = list()\n",
    "        for t in indic_tokenize.trivial_tokenize(line):\n",
    "            tokens.append(t)\n",
    "        line = tokens\n",
    "        line = [ word.lower() for word in line]\n",
    "        line = [word for word in line if not re.search(r'\\d', word)]\n",
    "        line = ' '.join(line)\n",
    "        clean_hin_lines.append(line)\n",
    "    return clean_hin_lines    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbaa487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "इसे दोबारा पढ़ें।\n",
      "इसे दोबारा पढ़ें\n"
     ]
    }
   ],
   "source": [
    "clean_hindi_lines = clean_hindi_data(hindi_sentences)\n",
    "print(hindi_sentences[133])\n",
    "print(clean_hindi_lines[133])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a2a0e",
   "metadata": {},
   "source": [
    "### Adding SOS and EOS ,  PREPARING INPUTS FOR ENCODER AND DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 2200 #(about 80-20 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4905d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad940c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input sentences\n",
    "input_sentences = clean_hindi_lines[0:num_sentences]\n",
    "\n",
    "#output sentences\n",
    "for line in clean_eng_lines[0:num_sentences]:\n",
    "    line = line + ' <eos>'\n",
    "    output_sentences.append(line)\n",
    "\n",
    "#output sentence input\n",
    "\n",
    "for line in clean_eng_lines[0:num_sentences]:\n",
    "    line = '<sos> ' + line\n",
    "    output_sentences_inputs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb96624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> wow',\n",
       " '<sos> help',\n",
       " '<sos> jump',\n",
       " '<sos> jump',\n",
       " '<sos> jump',\n",
       " '<sos> hello',\n",
       " '<sos> hello',\n",
       " '<sos> cheers',\n",
       " '<sos> cheers',\n",
       " '<sos> got it']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences_inputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ed7862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 2200\n",
      "num samples output: 2200\n",
      "num samples output input: 2200\n"
     ]
    }
   ],
   "source": [
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f712c",
   "metadata": {},
   "source": [
    "#### Tokenizing and padding  english and hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf104a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd9eaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe6b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "  def __init__(self, lang):\n",
    "    self.lang = lang\n",
    "    self.word2idx = {}\n",
    "    self.idx2word = {}\n",
    "    self.vocab = set()\n",
    "    \n",
    "    self.create_index()\n",
    "    \n",
    "  def create_index(self):\n",
    "    for phrase in self.lang:\n",
    "      self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "    self.vocab = sorted(self.vocab)\n",
    "    \n",
    "    self.word2idx['<sos>'] = 0\n",
    "#     self.word2idx['<eos>'] = 1\n",
    "    \n",
    "    for index, word in enumerate(self.vocab):\n",
    "      self.word2idx[word] = index + 1\n",
    "    \n",
    "    for word, index in self.word2idx.items():\n",
    "      self.idx2word[index] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8592191",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in range(len(input_sentences)):\n",
    "    pairs.append([])\n",
    "    pairs[i].append(input_sentences[i])\n",
    "    pairs[i].append(output_sentences_inputs[i])\n",
    "    pairs[i].append(output_sentences[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "120b2bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> wow',\n",
       " '<sos> help',\n",
       " '<sos> jump',\n",
       " '<sos> jump',\n",
       " '<sos> jump',\n",
       " '<sos> hello',\n",
       " '<sos> hello',\n",
       " '<sos> cheers',\n",
       " '<sos> cheers',\n",
       " '<sos> got it']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences_inputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "448fd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(pairs):  \n",
    "    inp_lang = LanguageIndex(hi for hi, en, en2 in pairs)\n",
    "    targ_lang = LanguageIndex(en for hi, en, en2 in pairs)\n",
    "#   targ_lang_decoder_output: english sentences with <eos>\n",
    "    targ_lang_decoder_output = LanguageIndex(en2 for hi, en, en2 in pairs)\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in hi.split(' ')] for hi, en, en2 in pairs]\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for hi, en, en2 in pairs]\n",
    "    target_tensor_decoder_output = [[targ_lang_decoder_output.word2idx[s] for s in en2.split(' ')] for hi, en, en2 in pairs]\n",
    "    max_length_inp, max_length_targ,  = max_length(input_tensor), max_length(target_tensor)\n",
    "    input_lang_vocab = inp_lang.vocab\n",
    "    return inp_lang, targ_lang,targ_lang_decoder_output, max_length_inp, max_length_targ, input_tensor, target_tensor, target_tensor_decoder_output, input_lang_vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28c26a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<sos>': 0, '<eos>': 1, 'a': 2, 'abandoned': 3, 'ability': 4, 'ablaze': 5, 'able': 6, 'about': 7, 'abroad': 8, 'absence': 9, 'absent': 10, 'absolute': 11, 'absurd': 12, 'abused': 13, 'accepted': 14, 'accident': 15, 'accidental': 16, 'accompanied': 17, 'accompany': 18, 'according': 19, 'account': 20, 'accused': 21, 'accustomed': 22, 'ache': 23, 'acquainted': 24, 'across': 25, 'actor': 26, 'actress': 27, 'add': 28, 'address': 29, 'admit': 30, 'adopted': 31, 'advantage': 32, 'advice': 33, 'advise': 34, 'advised': 35, 'afraid': 36, 'africa': 37, 'after': 38, 'afternoon': 39, 'again': 40, 'against': 41, 'age': 42, 'agree': 43, 'agreement': 44, 'air': 45, 'airport': 46, 'alcohol': 47, 'alive': 48, 'all': 49, 'allergic': 50, 'allow': 51, 'allowed': 52, 'almost': 53, 'alone': 54, 'along': 55, 'aloud': 56, 'already': 57, 'also': 58, 'always': 59, 'am': 60, 'amateur': 61, 'amazed': 62, 'ambulance': 63, 'america': 64, 'american': 65, 'among': 66, 'amount': 67, 'an': 68, 'anchorage': 69, 'and': 70, 'angry': 71, 'animal': 72, 'animals': 73, 'annoys': 74, 'another': 75, 'answer': 76, 'answered': 77, 'answering': 78, 'anxious': 79, 'any': 80, 'anybody': 81, 'anymore': 82, 'anyone': 83, 'anything': 84, 'anywhere': 85, 'apart': 86, 'apartment': 87, 'apology': 88, 'appeared': 89, 'appearing': 90, 'appears': 91, 'apple': 92, 'apples': 93, 'applies': 94, 'appointment': 95, 'approached': 96, 'april': 97, 'arabic': 98, 'are': 99, 'arent': 100, 'arguing': 101, 'arms': 102, 'around': 103, 'arrest': 104, 'arrive': 105, 'arrived': 106, 'arrogance': 107, 'arrogant': 108, 'art': 109, 'as': 110, 'ashamed': 111, 'asia': 112, 'ask': 113, 'asked': 114, 'asking': 115, 'asleep': 116, 'assaulted': 117, 'at': 118, 'atheist': 119, 'attend': 120, 'attended': 121, 'attention': 122, 'august': 123, 'aunt': 124, 'australia': 125, 'author': 126, 'aware': 127, 'away': 128, 'awesome': 129, 'b': 130, 'baby': 131, 'back': 132, 'bad': 133, 'bag': 134, 'baggage': 135, 'bags': 136, 'ball': 137, 'bank': 138, 'bar': 139, 'barbers': 140, 'barking': 141, 'baseball': 142, 'basketball': 143, 'bath': 144, 'bathroom': 145, 'batter': 146, 'be': 147, 'beach': 148, 'bear': 149, 'beard': 150, 'beast': 151, 'beating': 152, 'beatles': 153, 'beautiful': 154, 'beautifully': 155, 'beauty': 156, 'became': 157, 'because': 158, 'become': 159, 'bed': 160, 'been': 161, 'beer': 162, 'before': 163, 'beg': 164, 'began': 165, 'begin': 166, 'beginnings': 167, 'begins': 168, 'begun': 169, 'behavior': 170, 'behind': 171, 'being': 172, 'believe': 173, 'believes': 174, 'bell': 175, 'bent': 176, 'bern': 177, 'besides': 178, 'best': 179, 'bet': 180, 'betrayed': 181, 'better': 182, 'between': 183, 'bicycle': 184, 'big': 185, 'bigger': 186, 'billion': 187, 'biology': 188, 'bird': 189, 'birds': 190, 'birthday': 191, 'bit': 192, 'bitter': 193, 'black': 194, 'blame': 195, 'bloody': 196, 'bloom': 197, 'blue': 198, 'board': 199, 'boat': 200, 'bombay': 201, 'bone': 202, 'bones': 203, 'book': 204, 'books': 205, 'bored': 206, 'born': 207, 'borrow': 208, 'boss': 209, 'boston': 210, 'both': 211, 'bottles': 212, 'bought': 213, 'bound': 214, 'bowls': 215, 'box': 216, 'boy': 217, 'boys': 218, 'branches': 219, 'bread': 220, 'breakfast': 221, 'breathe': 222, 'breathed': 223, 'bride': 224, 'bring': 225, 'broke': 226, 'broken': 227, 'brother': 228, 'brothers': 229, 'brought': 230, 'brown': 231, 'brush': 232, 'buddhism': 233, 'build': 234, 'buildings': 235, 'bullet': 236, 'bum': 237, 'bun': 238, 'buried': 239, 'burned': 240, 'burning': 241, 'burns': 242, 'burst': 243, 'bus': 244, 'bush': 245, 'business': 246, 'busy': 247, 'but': 248, 'butter': 249, 'buy': 250, 'by': 251, 'cafe': 252, 'cafeteria': 253, 'cage': 254, 'cake': 255, 'call': 256, 'called': 257, 'came': 258, 'camera': 259, 'can': 260, 'canada': 261, 'cancer': 262, 'cannot': 263, 'cant': 264, 'capital': 265, 'car': 266, 'care': 267, 'carried': 268, 'cars': 269, 'case': 270, 'cash': 271, 'cat': 272, 'catch': 273, 'catches': 274, 'catching': 275, 'cats': 276, 'caught': 277, 'causes': 278, 'cave': 279, 'ceased': 280, 'celebrated': 281, 'cemetery': 282, 'century': 283, 'chair': 284, 'chairs': 285, 'chance': 286, 'changed': 287, 'changing': 288, 'charge': 289, 'charged': 290, 'chat': 291, 'cheaper': 292, 'cheek': 293, 'cheers': 294, 'cheese': 295, 'child': 296, 'childhood': 297, 'children': 298, 'china': 299, 'chinese': 300, 'choose': 301, 'church': 302, 'city': 303, 'class': 304, 'classical': 305, 'classmates': 306, 'clean': 307, 'cleaned': 308, 'clear': 309, 'cleared': 310, 'clever': 311, 'cliff': 312, 'climb': 313, 'climbed': 314, 'clock': 315, 'close': 316, 'closed': 317, 'cloth': 318, 'clothes': 319, 'cloud': 320, 'club': 321, 'clung': 322, 'coal': 323, 'coffee': 324, 'coincidentally': 325, 'coins': 326, 'cold': 327, 'colds': 328, 'collecting': 329, 'college': 330, 'color': 331, 'come': 332, 'comes': 333, 'comfort': 334, 'coming': 335, 'committed': 336, 'common': 337, 'company': 338, 'complained': 339, 'complains': 340, 'completed': 341, 'completely': 342, 'computer': 343, 'concealed': 344, 'concern': 345, 'condition': 346, 'conference': 347, 'conform': 348, 'congratulations': 349, 'consciousness': 350, 'consented': 351, 'consider': 352, 'consists': 353, 'contain': 354, 'contains': 355, 'continued': 356, 'control': 357, 'convinced': 358, 'cook': 359, 'cooked': 360, 'cooking': 361, 'corner': 362, 'correct': 363, 'costly': 364, 'coughing': 365, 'could': 366, 'couldnt': 367, 'count': 368, 'counting': 369, 'country': 370, 'countryside': 371, 'courage': 372, 'cousin': 373, 'cover': 374, 'covered': 375, 'cradle': 376, 'crash': 377, 'cricket': 378, 'cried': 379, 'crime': 380, 'cross': 381, 'crossing': 382, 'crow': 383, 'crows': 384, 'crushed': 385, 'cry': 386, 'crying': 387, 'cup': 388, 'cups': 389, 'current': 390, 'cut': 391, 'cycling': 392, 'dad': 393, 'damp': 394, 'dance': 395, 'danger': 396, 'dangerous': 397, 'dark': 398, 'daughter': 399, 'daughters': 400, 'day': 401, 'days': 402, 'daytime': 403, 'dead': 404, 'deals': 405, 'dear': 406, 'death': 407, 'debt': 408, 'debts': 409, 'decay': 410, 'deceive': 411, 'decided': 412, 'decision': 413, 'decline': 414, 'declined': 415, 'deep': 416, 'deeply': 417, 'deer': 418, 'deers': 419, 'definitely': 420, 'degree': 421, 'delhi': 422, 'dentist': 423, 'depends': 424, 'deprived': 425, 'desk': 426, 'destroyed': 427, 'detail': 428, 'dialed': 429, 'diamond': 430, 'dictionary': 431, 'did': 432, 'didnt': 433, 'die': 434, 'died': 435, 'dies': 436, 'differ': 437, 'difficult': 438, 'digging': 439, 'dine': 440, 'dinner': 441, 'directions': 442, 'disappointed': 443, 'discussed': 444, 'discussing': 445, 'dishes': 446, 'dislike': 447, 'dissatisfied': 448, 'distance': 449, 'disturbed': 450, 'disturbing': 451, 'do': 452, 'doctor': 453, 'does': 454, 'doesnt': 455, 'dog': 456, 'dogs': 457, 'doing': 458, 'doll': 459, 'dollars': 460, 'done': 461, 'dont': 462, 'door': 463, 'doubt': 464, 'dove': 465, 'down': 466, 'dozen': 467, 'dozens': 468, 'dream': 469, 'dreamed': 470, 'dreams': 471, 'dress': 472, 'dressed': 473, 'dried': 474, 'drink': 475, 'drinkable': 476, 'drinking': 477, 'drinks': 478, 'drive': 479, 'driven': 480, 'drop': 481, 'drove': 482, 'due': 483, 'dug': 484, 'dust': 485, 'each': 486, 'eager': 487, 'ear': 488, 'earlier': 489, 'early': 490, 'earns': 491, 'ears': 492, 'earth': 493, 'easily': 494, 'easter': 495, 'easy': 496, 'eat': 497, 'eaten': 498, 'eating': 499, 'eats': 500, 'education': 501, 'eggs': 502, 'eight': 503, 'eightthirty': 504, 'electricity': 505, 'elephant': 506, 'elephants': 507, 'elevator': 508, 'eleven': 509, 'else': 510, 'embezzled': 511, 'employees': 512, 'employs': 513, 'empty': 514, 'end': 515, 'enemies': 516, 'enemy': 517, 'engaged': 518, 'engine': 519, 'english': 520, 'enjoyed': 521, 'enough': 522, 'envelope': 523, 'enveloped': 524, 'envied': 525, 'environment': 526, 'equal': 527, 'escaped': 528, 'europe': 529, 'even': 530, 'ever': 531, 'every': 532, 'everybody': 533, 'everyone': 534, 'everything': 535, 'exactly': 536, 'examination': 537, 'example': 538, 'excellent': 539, 'exciting': 540, 'excuse': 541, 'exhibited': 542, 'expect': 543, 'expenses': 544, 'expensive': 545, 'experiment': 546, 'explain': 547, 'explained': 548, 'exports': 549, 'extend': 550, 'extent': 551, 'eye': 552, 'eyes': 553, 'face': 554, 'facebook': 555, 'faces': 556, 'fact': 557, 'fail': 558, 'failure': 559, 'fainted': 560, 'fair': 561, 'fairies': 562, 'fall': 563, 'fallen': 564, 'families': 565, 'family': 566, 'famous': 567, 'fantastic': 568, 'far': 569, 'farm': 570, 'farther': 571, 'fast': 572, 'faster': 573, 'fat': 574, 'fate': 575, 'father': 576, 'fathers': 577, 'fault': 578, 'faults': 579, 'favor': 580, 'favorite': 581, 'fear': 582, 'feed': 583, 'feel': 584, 'feels': 585, 'feet': 586, 'fell': 587, 'felt': 588, 'fever': 589, 'few': 590, 'fewer': 591, 'fifteen': 592, 'fifth': 593, 'fifty': 594, 'fighting': 595, 'figured': 596, 'find': 597, 'fine': 598, 'finger': 599, 'finish': 600, 'finished': 601, 'finland': 602, 'fire': 603, 'fired': 604, 'first': 605, 'fish': 606, 'fit': 607, 'five': 608, 'fixed': 609, 'flames': 610, 'flat': 611, 'flew': 612, 'flowers': 613, 'fly': 614, 'flying': 615, 'folder': 616, 'follow': 617, 'followed': 618, 'follows': 619, 'fond': 620, 'food': 621, 'foods': 622, 'fool': 623, 'foot': 624, 'football': 625, 'for': 626, 'forehead': 627, 'foreign': 628, 'forest': 629, 'forget': 630, 'forgive': 631, 'forgot': 632, 'fortune': 633, 'found': 634, 'four': 635, 'fox': 636, 'france': 637, 'free': 638, 'french': 639, 'frequently': 640, 'fresh': 641, 'friday': 642, 'friend': 643, 'friends': 644, 'from': 645, 'front': 646, 'full': 647, 'fully': 648, 'fun': 649, 'funds': 650, 'furniture': 651, 'further': 652, 'future': 653, 'game': 654, 'garden': 655, 'gas': 656, 'gate': 657, 'gathered': 658, 'gave': 659, 'gazed': 660, 'gentle': 661, 'gentleman': 662, 'gently': 663, 'genuine': 664, 'german': 665, 'germany': 666, 'get': 667, 'gets': 668, 'getting': 669, 'ghosts': 670, 'girl': 671, 'give': 672, 'glad': 673, 'gladly': 674, 'glass': 675, 'glasses': 676, 'glimpse': 677, 'go': 678, 'god': 679, 'goes': 680, 'going': 681, 'gold': 682, 'golf': 683, 'gone': 684, 'good': 685, 'goodbye': 686, 'goodfornothing': 687, 'goods': 688, 'got': 689, 'governed': 690, 'grandfather': 691, 'grandmother': 692, 'grateful': 693, 'grave': 694, 'graveyard': 695, 'great': 696, 'greek': 697, 'greeks': 698, 'green': 699, 'group': 700, 'growling': 701, 'grown': 702, 'grows': 703, 'guitar': 704, 'gun': 705, 'guy': 706, 'guys': 707, 'gym': 708, 'habit': 709, 'had': 710, 'hair': 711, 'half': 712, 'hammered': 713, 'hand': 714, 'handle': 715, 'hands': 716, 'happen': 717, 'happened': 718, 'happily': 719, 'happiness': 720, 'happy': 721, 'hard': 722, 'harder': 723, 'hardly': 724, 'harm': 725, 'has': 726, 'hasnt': 727, 'haste': 728, 'hat': 729, 'hate': 730, 'hated': 731, 'hates': 732, 'haunted': 733, 'have': 734, 'havent': 735, 'having': 736, 'haze': 737, 'he': 738, 'head': 739, 'headache': 740, 'headaches': 741, 'health': 742, 'healthy': 743, 'hear': 744, 'heard': 745, 'heart': 746, 'heat': 747, 'heavily': 748, 'hell': 749, 'hello': 750, 'help': 751, 'helped': 752, 'helps': 753, 'hens': 754, 'her': 755, 'here': 756, 'hers': 757, 'herself': 758, 'hes': 759, 'hesitated': 760, 'hesitation': 761, 'hid': 762, 'high': 763, 'him': 764, 'his': 765, 'history': 766, 'hit': 767, 'hobby': 768, 'holding': 769, 'hole': 770, 'home': 771, 'homework': 772, 'honest': 773, 'honesty': 774, 'hope': 775, 'horse': 776, 'hospital': 777, 'hot': 778, 'hotel': 779, 'hour': 780, 'hours': 781, 'house': 782, 'houses': 783, 'housework': 784, 'how': 785, 'hows': 786, 'hundred': 787, 'hunger': 788, 'hungry': 789, 'hunting': 790, 'hurried': 791, 'hurry': 792, 'hurt': 793, 'husband': 794, 'i': 795, 'id': 796, 'idea': 797, 'if': 798, 'ignorance': 799, 'ignore': 800, 'ill': 801, 'illness': 802, 'im': 803, 'imagine': 804, 'imitate': 805, 'impatient': 806, 'import': 807, 'important': 808, 'in': 809, 'includes': 810, 'income': 811, 'increasing': 812, 'independence': 813, 'india': 814, 'informed': 815, 'ink': 816, 'innocence': 817, 'innocent': 818, 'insecure': 819, 'insert': 820, 'inside': 821, 'instead': 822, 'intelligent': 823, 'interested': 824, 'interesting': 825, 'interpret': 826, 'into': 827, 'introduce': 828, 'investigation': 829, 'invitation': 830, 'iron': 831, 'is': 832, 'island': 833, 'isnt': 834, 'it': 835, 'italy': 836, 'its': 837, 'itself': 838, 'ive': 839, 'jacket': 840, 'jam': 841, 'japan': 842, 'japanese': 843, 'jeans': 844, 'job': 845, 'jobs': 846, 'jogging': 847, 'john': 848, 'join': 849, 'joke': 850, 'joy': 851, 'jump': 852, 'june': 853, 'jungle': 854, 'just': 855, 'keep': 856, 'keeps': 857, 'kept': 858, 'key': 859, 'keys': 860, 'kidding': 861, 'kids': 862, 'killed': 863, 'kilo': 864, 'kind': 865, 'king': 866, 'kite': 867, 'kites': 868, 'knew': 869, 'knife': 870, 'knocked': 871, 'knocking': 872, 'know': 873, 'known': 874, 'knows': 875, 'koala': 876, 'korea': 877, 'kyoto': 878, 'lack': 879, 'lacks': 880, 'laid': 881, 'lake': 882, 'land': 883, 'language': 884, 'languages': 885, 'large': 886, 'larger': 887, 'last': 888, 'late': 889, 'lately': 890, 'later': 891, 'laugh': 892, 'laughed': 893, 'law': 894, 'lay': 895, 'lazy': 896, 'lean': 897, 'leaped': 898, 'learn': 899, 'learned': 900, 'learning': 901, 'leather': 902, 'leave': 903, 'leaves': 904, 'left': 905, 'leg': 906, 'legs': 907, 'lent': 908, 'less': 909, 'let': 910, 'lets': 911, 'letter': 912, 'leveled': 913, 'lie': 914, 'life': 915, 'light': 916, 'lights': 917, 'like': 918, 'liked': 919, 'likes': 920, 'limit': 921, 'lincoln': 922, 'line': 923, 'lion': 924, 'lions': 925, 'listen': 926, 'listened': 927, 'literature': 928, 'little': 929, 'live': 930, 'lived': 931, 'lives': 932, 'living': 933, 'london': 934, 'lonely': 935, 'long': 936, 'longer': 937, 'look': 938, 'looked': 939, 'looking': 940, 'looks': 941, 'lose': 942, 'lost': 943, 'lot': 944, 'louder': 945, 'love': 946, 'loved': 947, 'loves': 948, 'low': 949, 'loyalty': 950, 'lucky': 951, 'lunch': 952, 'machine': 953, 'made': 954, 'maid': 955, 'mail': 956, 'make': 957, 'makes': 958, 'making': 959, 'man': 960, 'managed': 961, 'manager': 962, 'many': 963, 'map': 964, 'market': 965, 'married': 966, 'marry': 967, 'mary': 968, 'marys': 969, 'match': 970, 'math': 971, 'matsumoto': 972, 'matter': 973, 'matters': 974, 'may': 975, 'me': 976, 'meal': 977, 'meals': 978, 'meant': 979, 'medicine': 980, 'meet': 981, 'meeting': 982, 'memory': 983, 'men': 984, 'mentioned': 985, 'message': 986, 'met': 987, 'metal': 988, 'mexico': 989, 'might': 990, 'mile': 991, 'miles': 992, 'milk': 993, 'million': 994, 'mind': 995, 'mine': 996, 'minutes': 997, 'miracle': 998, 'miss': 999, 'missed': 1000, 'missing': 1001, 'mist': 1002, 'mistake': 1003, 'mistakes': 1004, 'mohan': 1005, 'mom': 1006, 'moment': 1007, 'monday': 1008, 'money': 1009, 'monkey': 1010, 'monsoon': 1011, 'month': 1012, 'months': 1013, 'moon': 1014, 'more': 1015, 'morning': 1016, 'most': 1017, 'mother': 1018, 'motioned': 1019, 'mountain': 1020, 'mouse': 1021, 'mouth': 1022, 'move': 1023, 'moves': 1024, 'movie': 1025, 'movies': 1026, 'much': 1027, 'mud': 1028, 'murder': 1029, 'murders': 1030, 'music': 1031, 'must': 1032, 'my': 1033, 'myself': 1034, 'mystery': 1035, 'name': 1036, 'narrow': 1037, 'nationality': 1038, 'naughty': 1039, 'nauseous': 1040, 'near': 1041, 'nearby': 1042, 'neat': 1043, 'necessary': 1044, 'need': 1045, 'needs': 1046, 'neighborhood': 1047, 'neighbors': 1048, 'neither': 1049, 'nerve': 1050, 'nerves': 1051, 'nests': 1052, 'never': 1053, 'new': 1054, 'news': 1055, 'newspaper': 1056, 'newspapers': 1057, 'next': 1058, 'nextdoor': 1059, 'nice': 1060, 'night': 1061, 'nine': 1062, 'ninth': 1063, 'no': 1064, 'nobody': 1065, 'nobodys': 1066, 'noise': 1067, 'nonsense': 1068, 'noon': 1069, 'not': 1070, 'notebook': 1071, 'nothing': 1072, 'notice': 1073, 'novel': 1074, 'now': 1075, 'nowadays': 1076, 'number': 1077, 'nuts': 1078, 'obliged': 1079, 'observe': 1080, 'obstinate': 1081, 'occasionally': 1082, 'occurred': 1083, 'occurs': 1084, 'ocean': 1085, 'oclock': 1086, 'odd': 1087, 'of': 1088, 'off': 1089, 'offer': 1090, 'offered': 1091, 'office': 1092, 'officer': 1093, 'often': 1094, 'oh': 1095, 'oil': 1096, 'ok': 1097, 'old': 1098, 'older': 1099, 'on': 1100, 'once': 1101, 'one': 1102, 'ones': 1103, 'only': 1104, 'open': 1105, 'opening': 1106, 'opinions': 1107, 'opportunity': 1108, 'or': 1109, 'oranges': 1110, 'order': 1111, 'ordered': 1112, 'orphan': 1113, 'osaka': 1114, 'other': 1115, 'others': 1116, 'our': 1117, 'ours': 1118, 'ourselves': 1119, 'out': 1120, 'outside': 1121, 'over': 1122, 'overseas': 1123, 'owe': 1124, 'own': 1125, 'owner': 1126, 'oxford': 1127, 'pack': 1128, 'page': 1129, 'pain': 1130, 'paint': 1131, 'painted': 1132, 'pair': 1133, 'paper': 1134, 'parallel': 1135, 'pardon': 1136, 'parents': 1137, 'paris': 1138, 'park': 1139, 'party': 1140, 'passed': 1141, 'passport': 1142, 'past': 1143, 'patient': 1144, 'patrolling': 1145, 'pay': 1146, 'peace': 1147, 'peacefully': 1148, 'peasant': 1149, 'pen': 1150, 'pencil': 1151, 'pencils': 1152, 'people': 1153, 'perfect': 1154, 'perhaps': 1155, 'person': 1156, 'persuade': 1157, 'pet': 1158, 'phone': 1159, 'phrase': 1160, 'physics': 1161, 'pianist': 1162, 'piano': 1163, 'pick': 1164, 'picture': 1165, 'pictures': 1166, 'pie': 1167, 'piece': 1168, 'pieces': 1169, 'pigeons': 1170, 'pink': 1171, 'pipe': 1172, 'pity': 1173, 'place': 1174, 'plan': 1175, 'plane': 1176, 'plans': 1177, 'planted': 1178, 'planting': 1179, 'plants': 1180, 'play': 1181, 'player': 1182, 'playing': 1183, 'plays': 1184, 'please': 1185, 'pleased': 1186, 'plenty': 1187, 'plus': 1188, 'pm': 1189, 'pocket': 1190, 'poem': 1191, 'point': 1192, 'poisonous': 1193, 'pole': 1194, 'police': 1195, 'policeman': 1196, 'policy': 1197, 'politics': 1198, 'pond': 1199, 'poor': 1200, 'population': 1201, 'possible': 1202, 'potatoes': 1203, 'pounds': 1204, 'pours': 1205, 'power': 1206, 'powers': 1207, 'prattles': 1208, 'prefer': 1209, 'prefers': 1210, 'prepared': 1211, 'preparing': 1212, 'present': 1213, 'pressed': 1214, 'pretending': 1215, 'pretty': 1216, 'prevented': 1217, 'pride': 1218, 'prided': 1219, 'prisoner': 1220, 'prize': 1221, 'problem': 1222, 'problems': 1223, 'profession': 1224, 'profit': 1225, 'program': 1226, 'promise': 1227, 'promote': 1228, 'prompt': 1229, 'properly': 1230, 'proposal': 1231, 'protect': 1232, 'proud': 1233, 'prove': 1234, 'proved': 1235, 'proverb': 1236, 'pulled': 1237, 'pulse': 1238, 'pumpkin': 1239, 'purchases': 1240, 'purse': 1241, 'put': 1242, 'question': 1243, 'questions': 1244, 'quick': 1245, 'quickly': 1246, 'quit': 1247, 'quite': 1248, 'quitting': 1249, 'quota': 1250, 'rabbit': 1251, 'race': 1252, 'radio': 1253, 'rain': 1254, 'raining': 1255, 'rains': 1256, 'rainy': 1257, 'raise': 1258, 'raised': 1259, 'ran': 1260, 'rang': 1261, 'rarely': 1262, 'rather': 1263, 'read': 1264, 'reading': 1265, 'reads': 1266, 'ready': 1267, 'real': 1268, 'realized': 1269, 'really': 1270, 'reap': 1271, 'reasonable': 1272, 'rebellion': 1273, 'received': 1274, 'red': 1275, 'refund': 1276, 'refused': 1277, 'regret': 1278, 'released': 1279, 'remain': 1280, 'remained': 1281, 'remains': 1282, 'remember': 1283, 'reminds': 1284, 'remorse': 1285, 'remote': 1286, 'remove': 1287, 'repair': 1288, 'repeated': 1289, 'reply': 1290, 'reproached': 1291, 'rescued': 1292, 'resolved': 1293, 'rest': 1294, 'rested': 1295, 'result': 1296, 'results': 1297, 'return': 1298, 'reverse': 1299, 'rice': 1300, 'rich': 1301, 'rid': 1302, 'ride': 1303, 'right': 1304, 'ringing': 1305, 'risks': 1306, 'river': 1307, 'road': 1308, 'robbed': 1309, 'rodicas': 1310, 'rome': 1311, 'roof': 1312, 'room': 1313, 'rooms': 1314, 'rope': 1315, 'roses': 1316, 'rotates': 1317, 'rotten': 1318, 'round': 1319, 'rude': 1320, 'rudeness': 1321, 'rule': 1322, 'rules': 1323, 'rumor': 1324, 'run': 1325, 'rung': 1326, 'running': 1327, 'russian': 1328, 'sadness': 1329, 'said': 1330, 'sail': 1331, 'salary': 1332, 'sale': 1333, 'same': 1334, 'sand': 1335, 'sandwiches': 1336, 'sang': 1337, 'sat': 1338, 'satisfied': 1339, 'save': 1340, 'saved': 1341, 'saver': 1342, 'saw': 1343, 'say': 1344, 'saying': 1345, 'says': 1346, 'scale': 1347, 'scar': 1348, 'scared': 1349, 'scattered': 1350, 'school': 1351, 'schools': 1352, 'science': 1353, 'scissors': 1354, 'scolded': 1355, 'sea': 1356, 'season': 1357, 'seat': 1358, 'secret': 1359, 'see': 1360, 'seem': 1361, 'seemed': 1362, 'seems': 1363, 'seen': 1364, 'seicho': 1365, 'send': 1366, 'sense': 1367, 'sentence': 1368, 'sentenced': 1369, 'sentences': 1370, 'servant': 1371, 'set': 1372, 'settle': 1373, 'several': 1374, 'shade': 1375, 'shall': 1376, 'shared': 1377, 'sharp': 1378, 'sharpshooter': 1379, 'shaving': 1380, 'she': 1381, 'sheer': 1382, 'sheets': 1383, 'shes': 1384, 'ship': 1385, 'shirt': 1386, 'shirts': 1387, 'shoes': 1388, 'shooting': 1389, 'shop': 1390, 'short': 1391, 'shot': 1392, 'should': 1393, 'shoulder': 1394, 'shouldnt': 1395, 'shouldve': 1396, 'shout': 1397, 'shouted': 1398, 'show': 1399, 'showed': 1400, 'shows': 1401, 'shrank': 1402, 'shut': 1403, 'sick': 1404, 'sickness': 1405, 'sight': 1406, 'sign': 1407, 'silent': 1408, 'silk': 1409, 'silky': 1410, 'similar': 1411, 'simple': 1412, 'sin': 1413, 'since': 1414, 'sing': 1415, 'singing': 1416, 'single': 1417, 'sinking': 1418, 'sister': 1419, 'sisters': 1420, 'sit': 1421, 'six': 1422, 'sixteenth': 1423, 'sixty': 1424, 'size': 1425, 'skating': 1426, 'skies': 1427, 'sky': 1428, 'slap': 1429, 'slave': 1430, 'sleep': 1431, 'sleeping': 1432, 'sleepy': 1433, 'slow': 1434, 'slowly': 1435, 'small': 1436, 'smell': 1437, 'smelled': 1438, 'smelling': 1439, 'smile': 1440, 'smiled': 1441, 'smog': 1442, 'smoke': 1443, 'smoking': 1444, 'smooth': 1445, 'snakes': 1446, 'snow': 1447, 'snowing': 1448, 'so': 1449, 'soccer': 1450, 'socks': 1451, 'sofa': 1452, 'soft': 1453, 'sold': 1454, 'soldiers': 1455, 'solve': 1456, 'solved': 1457, 'some': 1458, 'someday': 1459, 'someone': 1460, 'something': 1461, 'sometimes': 1462, 'somewhere': 1463, 'son': 1464, 'song': 1465, 'soon': 1466, 'sooner': 1467, 'sorry': 1468, 'sort': 1469, 'sound': 1470, 'soup': 1471, 'south': 1472, 'sow': 1473, 'spa': 1474, 'spanish': 1475, 'speak': 1476, 'speaking': 1477, 'speaks': 1478, 'specialty': 1479, 'speech': 1480, 'speed': 1481, 'spend': 1482, 'spoil': 1483, 'spoken': 1484, 'spread': 1485, 'squirrel': 1486, 'stairs': 1487, 'stamps': 1488, 'stand': 1489, 'standing': 1490, 'stars': 1491, 'start': 1492, 'started': 1493, 'starting': 1494, 'startled': 1495, 'starts': 1496, 'state': 1497, 'statement': 1498, 'station': 1499, 'stay': 1500, 'stayed': 1501, 'staying': 1502, 'steal': 1503, 'still': 1504, 'stingy': 1505, 'stolen': 1506, 'stomach': 1507, 'stomachache': 1508, 'stone': 1509, 'stones': 1510, 'stood': 1511, 'stop': 1512, 'stopped': 1513, 'store': 1514, 'stories': 1515, 'storm': 1516, 'story': 1517, 'strange': 1518, 'stranger': 1519, 'street': 1520, 'string': 1521, 'stripped': 1522, 'strong': 1523, 'struck': 1524, 'stubborn': 1525, 'stuck': 1526, 'student': 1527, 'students': 1528, 'studied': 1529, 'study': 1530, 'studying': 1531, 'stuff': 1532, 'stuttering': 1533, 'subject': 1534, 'subway': 1535, 'succeed': 1536, 'success': 1537, 'successful': 1538, 'such': 1539, 'sudden': 1540, 'suddenly': 1541, 'suffers': 1542, 'sugar': 1543, 'suit': 1544, 'suitcase': 1545, 'summer': 1546, 'sun': 1547, 'sunday': 1548, 'sundays': 1549, 'supper': 1550, 'sure': 1551, 'surprised': 1552, 'sweat': 1553, 'swim': 1554, 'swimming': 1555, 'switzerland': 1556, 'swollen': 1557, 'table': 1558, 'take': 1559, 'taken': 1560, 'taking': 1561, 'talk': 1562, 'talked': 1563, 'talking': 1564, 'talks': 1565, 'tall': 1566, 'tame': 1567, 'taste': 1568, 'tastes': 1569, 'taught': 1570, 'tax': 1571, 'taxi': 1572, 'tea': 1573, 'teach': 1574, 'teacher': 1575, 'teachers': 1576, 'teaching': 1577, 'team': 1578, 'teeth': 1579, 'telephone': 1580, 'television': 1581, 'tell': 1582, 'temper': 1583, 'temperature': 1584, 'temples': 1585, 'ten': 1586, 'tends': 1587, 'tennis': 1588, 'terminal': 1589, 'than': 1590, 'thank': 1591, 'that': 1592, 'thats': 1593, 'the': 1594, 'theft': 1595, 'their': 1596, 'them': 1597, 'themselves': 1598, 'then': 1599, 'there': 1600, 'theres': 1601, 'these': 1602, 'they': 1603, 'thick': 1604, 'thief': 1605, 'thin': 1606, 'thing': 1607, 'things': 1608, 'think': 1609, 'thinking': 1610, 'thirsty': 1611, 'thirty': 1612, 'this': 1613, 'those': 1614, 'thought': 1615, 'three': 1616, 'throw': 1617, 'throwing': 1618, 'thrown': 1619, 'ticket': 1620, 'tickets': 1621, 'tidy': 1622, 'tie': 1623, 'tiger': 1624, 'time': 1625, 'times': 1626, 'tiny': 1627, 'tired': 1628, 'to': 1629, 'tobacco': 1630, 'today': 1631, 'together': 1632, 'toilet': 1633, 'tokyo': 1634, 'told': 1635, 'tolerate': 1636, 'tom': 1637, 'tomorrow': 1638, 'toms': 1639, 'tonight': 1640, 'too': 1641, 'took': 1642, 'top': 1643, 'tore': 1644, 'touch': 1645, 'toward': 1646, 'towel': 1647, 'towels': 1648, 'town': 1649, 'toy': 1650, 'toyota': 1651, 'tracks': 1652, 'traffic': 1653, 'train': 1654, 'traitor': 1655, 'travel': 1656, 'travels': 1657, 'treasure': 1658, 'treats': 1659, 'tree': 1660, 'trees': 1661, 'tried': 1662, 'tries': 1663, 'trip': 1664, 'trouble': 1665, 'true': 1666, 'truly': 1667, 'trust': 1668, 'truth': 1669, 'try': 1670, 'trying': 1671, 'tuesday': 1672, 'turn': 1673, 'turned': 1674, 'turning': 1675, 'tv': 1676, 'twenty': 1677, 'twentyfour': 1678, 'twice': 1679, 'two': 1680, 'umbrella': 1681, 'unbelievable': 1682, 'uncle': 1683, 'under': 1684, 'underlined': 1685, 'understand': 1686, 'understands': 1687, 'undone': 1688, 'unfounded': 1689, 'unicycle': 1690, 'university': 1691, 'unless': 1692, 'unsolved': 1693, 'until': 1694, 'unused': 1695, 'up': 1696, 'us': 1697, 'use': 1698, 'used': 1699, 'useful': 1700, 'usual': 1701, 'usually': 1702, 'vain': 1703, 'vary': 1704, 'vase': 1705, 'vegetables': 1706, 'veracity': 1707, 'vertical': 1708, 'very': 1709, 'victory': 1710, 'village': 1711, 'visit': 1712, 'visited': 1713, 'visiting': 1714, 'voice': 1715, 'volume': 1716, 'vomiting': 1717, 'wait': 1718, 'waited': 1719, 'waiting': 1720, 'wake': 1721, 'walk': 1722, 'walked': 1723, 'walking': 1724, 'walks': 1725, 'wall': 1726, 'wallet': 1727, 'want': 1728, 'wanted': 1729, 'wants': 1730, 'war': 1731, 'warned': 1732, 'was': 1733, 'wash': 1734, 'washed': 1735, 'wasnt': 1736, 'waste': 1737, 'watch': 1738, 'watching': 1739, 'water': 1740, 'watering': 1741, 'way': 1742, 'we': 1743, 'wealthy': 1744, 'wearing': 1745, 'wears': 1746, 'weather': 1747, 'wedding': 1748, 'week': 1749, 'weeks': 1750, 'weighed': 1751, 'weird': 1752, 'welcome': 1753, 'well': 1754, 'went': 1755, 'were': 1756, 'western': 1757, 'weve': 1758, 'what': 1759, 'whatever': 1760, 'whatre': 1761, 'whats': 1762, 'wheel': 1763, 'when': 1764, 'where': 1765, 'wheres': 1766, 'wherever': 1767, 'which': 1768, 'whichever': 1769, 'while': 1770, 'white': 1771, 'who': 1772, 'whoever': 1773, 'whole': 1774, 'whos': 1775, 'whose': 1776, 'why': 1777, 'widely': 1778, 'wife': 1779, 'wild': 1780, 'will': 1781, 'willing': 1782, 'win': 1783, 'wind': 1784, 'window': 1785, 'wine': 1786, 'wings': 1787, 'winter': 1788, 'wipe': 1789, 'wiped': 1790, 'wish': 1791, 'wishes': 1792, 'with': 1793, 'withered': 1794, 'within': 1795, 'without': 1796, 'witnessed': 1797, 'woman': 1798, 'women': 1799, 'won': 1800, 'wonder': 1801, 'wonderful': 1802, 'wont': 1803, 'wood': 1804, 'wooden': 1805, 'wool': 1806, 'word': 1807, 'words': 1808, 'wore': 1809, 'work': 1810, 'worked': 1811, 'workers': 1812, 'working': 1813, 'works': 1814, 'world': 1815, 'worlds': 1816, 'worry': 1817, 'worrying': 1818, 'worse': 1819, 'worthless': 1820, 'would': 1821, 'wouldnt': 1822, 'wow': 1823, 'write': 1824, 'writes': 1825, 'written': 1826, 'wrong': 1827, 'wrote': 1828, 'year': 1829, 'years': 1830, 'yen': 1831, 'yes': 1832, 'yesterday': 1833, 'yet': 1834, 'york': 1835, 'you': 1836, 'youll': 1837, 'young': 1838, 'younger': 1839, 'youngster': 1840, 'your': 1841, 'youre': 1842, 'yours': 1843, 'yourself': 1844, 'youth': 1845, 'youve': 1846, 'zoo': 1847}\n"
     ]
    }
   ],
   "source": [
    "inp_lang, targ_lang, targ_lang_decoder_output, max_length_inp, max_length_targ , input_tensor , target_tensor,target_tensor_decoder_output, input_lang_vocab = load_dataset(pairs)\n",
    "print(targ_lang_decoder_output.word2idx)\n",
    "input_lang_word2idx = inp_lang.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43840fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "562efd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_tensor(max_length_inp, max_length_tar, inp_lang, targ_lang , input_tensor , target_tensor):\n",
    "    \n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp, padding='pre')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar, padding='post')\n",
    "    return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef620c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, target_tensor_train = padded_tensor(max_length_inp, max_length_targ, inp_lang, targ_lang ,input_tensor , target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5eafc8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 1928],\n",
       "       [   0,    0,    0, ...,    0,    0, 1393],\n",
       "       [   0,    0,    0, ...,    0,    0,  182],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1270,  314, 2241],\n",
       "       [   0,    0,    0, ..., 1737, 1560,  395],\n",
       "       [   0,    0,    0, ..., 1470,   26, 2234]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de3179d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 1823,    0, ...,    0,    0,    0],\n",
       "       [   1,  751,    0, ...,    0,    0,    0],\n",
       "       [   1,  852,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1, 1836, 1032, ...,  321,    0,    0],\n",
       "       [   1, 1846,  689, ...,    2, 1607,    0],\n",
       "       [   1, 1841,  537, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58b2a3",
   "metadata": {},
   "source": [
    "### word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3598187a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: Devanagari",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-a548af43f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membeddings_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mglove_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi-d100-glove.txt'\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Devanagari'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: Devanagari"
     ]
    }
   ],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('hi-d100-glove.txt' ,  encoding = 'utf-8')\n",
    "# glove_file = open('hi-d100-glove.txt' ,  encoding = 'Devanagari')\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:] , dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ace71e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb60eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.6.0",
   "language": "python",
   "name": "tf2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
